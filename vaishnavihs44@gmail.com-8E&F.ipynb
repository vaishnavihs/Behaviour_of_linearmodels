{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 9E and 9F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 9E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 5) (3200,)\n",
      "(800, 5) (800,)\n",
      "(1000, 5) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=24)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h43kDT3M41u5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can write your code here\n",
    "gamma = 0.001\n",
    "clf = SVC(gamma=gamma, C=100)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(xq):\n",
    "    val = 0\n",
    "    for alpha,xi in zip(clf.dual_coef_[0],clf.support_vectors_): #the dual_coef_[i] contains label[i]*alpha[i]\n",
    "        val += alpha*np.exp(-gamma*np.linalg.norm(xi-xq)**2) \n",
    "    return val+clf.intercept_.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_fun(X_val):\n",
    "    fcv = []\n",
    "    for xq in X_val:\n",
    "        fcv.append(K(xq))\n",
    "    return(np.array(fcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.69065026, -4.01123357, -2.48966713,  1.35046624, -2.59528514])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_fun(X_val)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.69065026, -4.01123357, -2.48966713,  1.35046624, -2.59528514])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_val)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hence both the values matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 9F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK- F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [550 250]\n"
     ]
    }
   ],
   "source": [
    "fcv = dec_fun(X_val)\n",
    "y_val_cpy = y_val.astype('float')\n",
    "unique,counts = np.unique(y_val_cpy,return_counts=True)\n",
    "print(unique,counts)\n",
    "y_val_cpy[y_val_cpy==unique[0]]=1.0/(counts[0]+2)\n",
    "y_val_cpy[y_val_cpy==unique[1]]=(counts[1]+1.0)/(counts[1]+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w,x,b):\n",
    "    return 1/(1+np.exp(-(np.dot(x,w.T)+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(X,y,w,b,lamda,alpha,N):\n",
    "    w_new = (1-alpha*lamda/N)*w + alpha*X*(y-sigmoid(w,X.T,b))\n",
    "    b_new = b + alpha*(y-sigmoid(w,X.T,b))\n",
    "    return w_new,b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, y, batchSize):\n",
    "    # loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "    for i in np.arange(0, X.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (X[i:i + batchSize], y[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_loss(A,n):# your code\n",
    "    loss=0\n",
    "    for Y in A:\n",
    "        loss += Y[0]*math.log(Y[1])+(1-Y[0])*math.log(1-Y[1]) \n",
    "    loss = -loss/n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros((1,))\n",
    "b = 0\n",
    "lamda  = 0.0001\n",
    "alpha = 0.0001\n",
    "N = len(fcv)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1\n",
      "Training Loss:0.23493825426267267\n",
      "Test Loss:0.2425100230708235\n",
      "===========================================================================\n",
      "iteration:2\n",
      "Training Loss:0.23249327107731976\n",
      "Test Loss:0.24023840043935585\n",
      "===========================================================================\n",
      "iteration:3\n",
      "Training Loss:0.2301828022467933\n",
      "Test Loss:0.2380944397637579\n",
      "===========================================================================\n",
      "iteration:4\n",
      "Training Loss:0.2279960983825657\n",
      "Test Loss:0.2360680256576708\n",
      "===========================================================================\n",
      "iteration:5\n",
      "Training Loss:0.22592352856194545\n",
      "Test Loss:0.23415008094321274\n",
      "===========================================================================\n",
      "iteration:6\n",
      "Training Loss:0.2239564385208662\n",
      "Test Loss:0.23233243693276473\n",
      "===========================================================================\n",
      "iteration:7\n",
      "Training Loss:0.2220870298487404\n",
      "Test Loss:0.23060772264397336\n",
      "===========================================================================\n",
      "iteration:8\n",
      "Training Loss:0.22030825665508055\n",
      "Test Loss:0.22896926981050644\n",
      "===========================================================================\n",
      "iteration:9\n",
      "Training Loss:0.21861373683729948\n",
      "Test Loss:0.2274110311293033\n",
      "===========================================================================\n",
      "iteration:10\n",
      "Training Loss:0.21699767560385105\n",
      "Test Loss:0.22592750964662947\n",
      "===========================================================================\n",
      "iteration:11\n",
      "Training Loss:0.2154547993265748\n",
      "Test Loss:0.22451369755555076\n",
      "===========================================================================\n",
      "iteration:12\n",
      "Training Loss:0.2139802981334635\n",
      "Test Loss:0.22316502297609517\n",
      "===========================================================================\n",
      "iteration:13\n",
      "Training Loss:0.21256977592563206\n",
      "Test Loss:0.22187730353132648\n",
      "===========================================================================\n",
      "iteration:14\n",
      "Training Loss:0.21121920672347122\n",
      "Test Loss:0.22064670572952833\n",
      "===========================================================================\n",
      "iteration:15\n",
      "Training Loss:0.20992489642733922\n",
      "Test Loss:0.21946970932372453\n",
      "===========================================================================\n",
      "iteration:16\n",
      "Training Loss:0.20868344922586715\n",
      "Test Loss:0.21834307595201283\n",
      "===========================================================================\n",
      "iteration:17\n",
      "Training Loss:0.2074917380064067\n",
      "Test Loss:0.21726382147118192\n",
      "===========================================================================\n",
      "iteration:18\n",
      "Training Loss:0.20634687822246328\n",
      "Test Loss:0.2162291914863361\n",
      "===========================================================================\n",
      "iteration:19\n",
      "Training Loss:0.20524620475607608\n",
      "Test Loss:0.21523663965420714\n",
      "===========================================================================\n",
      "iteration:20\n",
      "Training Loss:0.20418725138228666\n",
      "Test Loss:0.21428380840034117\n",
      "===========================================================================\n",
      "iteration:21\n",
      "Training Loss:0.20316773250057668\n",
      "Test Loss:0.21336851174268642\n",
      "===========================================================================\n",
      "iteration:22\n",
      "Training Loss:0.20218552684655522\n",
      "Test Loss:0.21248871995798965\n",
      "===========================================================================\n",
      "iteration:23\n",
      "Training Loss:0.20123866293784637\n",
      "Test Loss:0.211642545864433\n",
      "===========================================================================\n",
      "iteration:24\n",
      "Training Loss:0.2003253060424407\n",
      "Test Loss:0.21082823252520413\n",
      "===========================================================================\n",
      "iteration:25\n",
      "Training Loss:0.19944374648679883\n",
      "Test Loss:0.21004414220418222\n",
      "===========================================================================\n",
      "iteration:26\n",
      "Training Loss:0.19859238914562405\n",
      "Test Loss:0.20928874642745074\n",
      "===========================================================================\n",
      "iteration:27\n",
      "Training Loss:0.1977697439761738\n",
      "Test Loss:0.20856061702356138\n",
      "===========================================================================\n",
      "iteration:28\n",
      "Training Loss:0.19697441747787792\n",
      "Test Loss:0.20785841803186708\n",
      "===========================================================================\n",
      "iteration:29\n",
      "Training Loss:0.1962051049733266\n",
      "Test Loss:0.20718089838234707\n",
      "===========================================================================\n",
      "Final Weights:\n",
      "[1.2017521]\n",
      "Final Intercept: [-0.08806594]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "lossHistoryTrain = []\n",
    "lossHistoryTest = []\n",
    "epochs = range(1,30)\n",
    "for epoch in epochs:\n",
    "    # initialize the total loss for the epoch\n",
    "    epochLossTrain = []\n",
    "    epochLossTest = []\n",
    "    # loop over our data in batches\n",
    "    for (batchX, batchY) in next_batch(fcv, y_val_cpy, 1):\n",
    "        preds = sigmoid(w,batchX,b)\n",
    "        loss = -(batchY*math.log(preds)+(1-batchY)*math.log(1-preds))\n",
    "        epochLossTrain.append(loss)\n",
    "        w, b = update_weights(batchX,batchY,w,b,lamda,alpha,N)\n",
    "        \n",
    "    avgLossTrain = np.average(epochLossTrain)\n",
    "    lossHistoryTrain.append(avgLossTrain)\n",
    "    print(\"iteration:{}\".format(epoch))\n",
    "    print(\"Training Loss:{}\".format(avgLossTrain))\n",
    "    y_pred = [sigmoid(w,x.reshape(-1,1),b) for x in dec_fun(X_test)]\n",
    "    avgLossTest = compute_log_loss(zip(y_test,y_pred),len(y_test))\n",
    "    lossHistoryTest.append(avgLossTest)\n",
    "    print(\"Test Loss:{}\".format(avgLossTest))\n",
    "    print('='*75)\n",
    "print('Final Weights:')\n",
    "print(w)\n",
    "print('Final Intercept:',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(lossHistoryTrain), color='darkblue', label='Log loss Training Data')\n",
    "plt.plot(np.array(lossHistoryTest), color='orange', label='Log loss Test Data')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Log loss curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
